---
title: "Lab 4"
author: "Meihe Liu"
output: pdf_document
date: "11:59PM March 10, 2021"
---

Load up the famous iris dataset. We are going to do a different prediction problem. Imagine the only input x is Species and you are trying to predict y which is Petal.Length. A reasonable prediction is the average petal length within each Species. Prove that this is the OLS model by fitting an appropriate `lm` and then using the predict function to verify.

```{r}
data(iris)
mod=lm(Petal.Length ~ Species,iris)
mean(iris$Petal.Length[iris$Species == "setosa" ])
mean(iris$Petal.Length[iris$Species == "versicolor" ])
mean(iris$Petal.Length[iris$Species == "virginica" ])
predict(mod,data.frame(Species = c("setosa")))
predict(mod,data.frame(Species = c("versicolor")))
predict(mod,data.frame(Species = c("virginica")))
```

Construct the design matrix with an intercept, $X$, without using `model.matrix`.

```{r}
X <- cbind(1,iris$Species=="versicolor", iris$Species=="virginica")
head(X)
```

Find the hat matrix $H$ for this regression.

```{r}
H = X %*% solve(t(X) %*% X) %*% t(X)
Matrix::rankMatrix(H)

```

Verify this hat matrix is symmetric using the `expect_equal` function in the package `testthat`.

```{r}
pacman::p_load(testthat)
expect_equal(H,t(H))
#no need of tolerance
```

Verify this hat matrix is idempotent using the `expect_equal` function in the package `testthat`.

```{r}
expect_equal(H,H %*% H)
```

Using the `diag` function, find the trace of the hat matrix.

```{r}
sum(diag(H))
#sum of trace is the rank
```

It turns out the trace of a hat matrix is the same as its rank! But we don't have time to prove these interesting and useful facts..

For masters students: create a matrix $X_\perp$.

```{r}
#ec
```

Using the hat matrix, compute the $\hat{y}$ vector and using the projection onto the residual space, compute the $e$ vector and verify they are orthogonal to each other.

```{r}
I = diag(nrow(iris))
y = iris$Petal.Length
yhat = H %*% y
e = (I-H) %*% y
t(e) %*% yhat
head(e)
Matrix::rankMatrix(I-H)
```

Compute SST, SSR and SSE and $R^2$ and then show that SST = SSR + SSE.

```{r}
#SSE = e %*% t(e) this gives n x n matrix with rank 1.
SSE = t(e) %*% e
ybar = mean(y)
SST = t(y - ybar) %*% (y - ybar)
Rsq = 1 - SSE/SST
SSR = t(yhat - ybar) %*% (yhat - ybar)
expect_equal(SSR+SSE,SST)
#var(y)
#var(e)
```

Find the angle $\theta$ between $y$ - $\bar{y}1$ and $\hat{y} - \bar{y}1$ and then verify that its cosine squared is the same as the $R^2$ from the previous problem.

```{r}
theta = acos(t(y - ybar) %*% (yhat - ybar) / sqrt(SST*SSR))
theta = (180/pi)
```

Project the $y$ vector onto each column of the $X$ matrix and test if the sum of these projections is the same as yhat.

```{r}
proj1 = (X[,1] %*% t(X[,1]) / as.numeric(t(X[,1]) %*% X[,1])) %*% y
proj2 = (X[,2] %*% t(X[,2]) / as.numeric(t(X[,2]) %*% X[,2])) %*% y
proj3 = (X[,3] %*% t(X[,3]) / as.numeric(t(X[,3]) %*% X[,3])) %*% y
#expect_equal(proj1+proj2+proj3, yhat)
#not equal
```

Construct the design matrix without an intercept, $X$, without using `model.matrix`.

```{r}
X2 <- cbind(as.numeric(iris$Species=="setosa"), iris$Species=="versicolor", iris$Species=="virginica" )
head(X2)
```

Find the OLS estimates using this design matrix. It should be the sample averages of the petal lengths within species.

```{r}
H2 = X2 %*% solve(t(X2) %*% X2) %*% t(X2)
yhat2 = H2 %*% y
unique(yhat)
unique(yhat2)


mean(y[iris$Species == "setosa"])
mean(y[iris$Species == "versicolor"])
mean(y[iris$Species == "virginica"])
```

Verify the hat matrix constructed from this design matrix is the same as the hat matrix constructed from the design matrix with the intercept. (Fact: orthogonal projection matrices are unique).

```{r}
hat = X2 %*% solve(t(X2) %*% X2) %*% t(X2)

expect_equal(hat,H2)
```

Project the $y$ vector onto each column of the $X$ matrix and test if the sum of these projections is the same as yhat.

```{r}
proj1 = ((X2[,1] %*% t(X2[,1]))/as.numeric(t(X2[,1]) %*% X2[,1])) %*% y
proj2 = ((X2[,2] %*% t(X2[,2]))/as.numeric(t(X2[,2]) %*% X2[,2])) %*% y
proj3 = ((X2[,3] %*% t(X2[,3]))/as.numeric(t(X2[,3]) %*% X2[,3])) %*% y
yhat = H %*% y
expect_equal(proj1+proj2+proj3 , yhat)

```

Convert this design matrix into $Q$, an orthonormal matrix.

```{r}
Q = qr.Q(qr(X2))
head(Q)
#verification
sum(Q[, 1]^2) 
sum(Q[, 2]^2) 
sum(Q[, 3]^2) 
Q[, 1] %*% Q[, 2] 
Q[, 1] %*% Q[, 3] 
Q[, 2] %*% Q[, 3] 

```

Project the $y$ vector onto each column of the $Q$ matrix and test if the sum of these projections is the same as yhat.

```{r}
proj1 = ((Q[,1] %*% t(Q[,1]))/as.numeric(t(Q[,1]) %*% Q[,1])) %*% y
proj2 = ((Q[,2] %*% t(Q[,2]))/as.numeric(t(Q[,2]) %*% Q[,2])) %*% y
proj3 = ((Q[,3] %*% t(Q[,3]))/as.numeric(t(Q[,3]) %*% Q[,3])) %*% y
yhat_Q = Q %*% t(Q) %*% y
expect_equal(yhat_Q , yhat2)
```

Find the $p=3$ linear OLS estimates if $Q$ is used as the design matrix using the `lm` method. Is the OLS solution the same as the OLS solution for $X$?

```{r}
mod2 = lm(y~0+Q)
coef(mod2)

mean(iris$Petal.Length[iris$Species=="setosa"])
mean(iris$Petal.Length[iris$Species=="versicolor"])
mean(iris$Petal.Length[iris$Species=="virginica"])
```

Use the predict function and ensure that the predicted values are the same for both linear models: the one created with $X$ as its design matrix and the one created with $Q$ as its design matrix.

```{r}
colnames(X2)<-c("setosa", "versicolor", "virginica")
mod3 = lm(y~0+X2)
unique(predict(mod3, data.frame(X2)))
mod4 = lm(y~0+Q)
unique(predict(mod4, data.frame(Q)))
```


Clear the workspace and load the boston housing data and extract $X$ and $y$. The dimensions are $n=506$ and $p=13$. Create a matrix that is $(p + 1) \times (p + 1)$ full of NA's. Label the columns the same columns as X. Do not label the rows. For the first row, find the OLS estimate of the $y$ regressed on the first column only and put that in the first entry. For the second row, find the OLS estimates of the $y$ regressed on the first and second columns of $X$ only and put them in the first and second entries. For the third row, find the OLS estimates of the $y$ regressed on the first, second and third columns of $X$ only and put them in the first, second and third entries, etc. For the last row, fill it with the full OLS estimates.

```{r}
rm(list=ls())
Boston <- MASS::Boston
one_vec = rep(1,nrow(Boston)) 
X = as.matrix(cbind(one_vec,Boston[,1:13]))
y = Boston[,14]
n=nrow(X)
df = ncol(X)
Matrix <- matrix(NA,nrow = df, ncol=df,dimnames = list(NULL,colnames(X)))

for(i in 1:ncol(Matrix)){
  b= array(data = NA, dim = ncol(Matrix))
  X_new = X[,1:i]
  X_new = as.matrix(X_new)
  b [1:i]= solve(t(X_new) %*% X_new) %*% t(X_new) %*% y
  Matrix[i, ] <- b
} 
Matrix


```

Why are the estimates changing from row to row as you add in more predictors?

Because every row is a different model with different number of features i.e. the first row has 0 features with intercept, the second row has one feature with the intercept,the third row has two features with the intercept....
We find that the values of the weights of a single features may vary when we change the number of features we fit the model on.

Create a vector of length $p+1$ and compute the R^2 values for each of the above models. 

```{r}
Rsq_array = array(dim = 14)
ybar = mean(y)
SST = sum((y-ybar)^2)
for(i in 1:nrow(Matrix)){
  b = c(Matrix[i,1:i], rep(0, nrow(Matrix)-i))
  yhat = X %*% b
  SSR = sum((yhat - ybar)^2)
  Rsq = SSR / SST
  Rsq_array[i] = Rsq
}
Rsq_array
```

Is R^2 monotonically increasing? Why?

Yes. Beacuse as we are adding more features the model will fit better on  data.